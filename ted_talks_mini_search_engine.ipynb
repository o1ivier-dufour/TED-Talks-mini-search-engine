{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11566899",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\olitk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import itertools\n",
    "import nltk\n",
    "import math\n",
    "import json\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pathlib import Path\n",
    "\n",
    "   \n",
    "def getDocumentCollection(csv_path):\n",
    "    \"\"\" Return the content of the csv file passed in parameter in a list format \"\"\"\n",
    "    csv_file_lines = []\n",
    "    with open(csv_path, 'r', encoding=\"utf8\") as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        for line in csv_reader:\n",
    "            # if there is an empty line just go to the next iteration\n",
    "            if line[0] == '':\n",
    "                continue\n",
    "            else:\n",
    "                content = ''\n",
    "                # \"clean\" the text of the line\n",
    "                for word in re.sub(\"[^\\w]\", \" \",  line[0]).split():\n",
    "                    content += cleanWord(word)+ ' '\n",
    "\n",
    "                #append the clean line\n",
    "                line[0] = content \n",
    "                csv_file_lines.append(line)\n",
    "    # return the list, except the headers   \n",
    "    return csv_file_lines[1:]\n",
    "\n",
    "\n",
    "def cleanWord(word):\n",
    "    \"\"\"cleans word (i.e. stemming, lowercase, removing stop words)\"\"\"   \n",
    "    ps = PorterStemmer()\n",
    "    # stem word + put in lowercase\n",
    "    word = ps.stem(word, to_lowercase=True)\n",
    "    # remove unrelevant words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    if word in stop_words:\n",
    "        return ''\n",
    "    return word\n",
    "        \n",
    "        \n",
    "        \n",
    "def createIndex(documentCollection):\n",
    "    \"\"\"\n",
    "    returns an index over the given documentCollection. Maps every word to a\n",
    "    a dictionnary whit all the document/URL who contains it, associated with\n",
    "    their tfidf score\n",
    "    \"\"\"\n",
    "\n",
    "    # before computing the index, check if its JSON file exists.\n",
    "    # if so, read JSON instead of recomputing\n",
    "    path = Path(\"index.json\")\n",
    "    if path.is_file():\n",
    "        with open(\"index.json\") as index_file:\n",
    "            index = json.load(index_file)\n",
    "        return index\n",
    "\n",
    "\n",
    "    allWords = set(getAllWordsOfDocumentCollection(documentCollection))\n",
    "    index = {}\n",
    "    # build the index with, for the moment, the number of occurences of the word\n",
    "    # in each document/URL\n",
    "    for word in allWords:\n",
    "        for document in documentCollection:\n",
    "            if word in document[0]:\n",
    "                tf = re.sub(\"[^\\w]\", \" \",  document[0]).split().count(word)  #tf = term frequency\n",
    "                if tf == 0: continue\n",
    "                if word not in index.keys():\n",
    "                    index[word] = {document[1]: tf}\n",
    "                index[word][document[1]] = tf\n",
    "   \n",
    "    N = len(documentCollection) # N = number of documents in the corpus (ted talks)\n",
    "    print('number all docs: ' + str(N))\n",
    "\n",
    "    # replace each occurences by the tfidf score\n",
    "    for word in index.keys():\n",
    "        numDocsContainingWord = len(index[word].keys())\n",
    "        if numDocsContainingWord == 0: \n",
    "            idf = 0\n",
    "        else: \n",
    "            idf = math.log(1+ N/numDocsContainingWord)\n",
    "        for doc in index[word].keys():\n",
    "            index[word][doc] = index[word][doc] * idf # tf * idf\n",
    "\n",
    "        \n",
    "    # write index as JSON file\n",
    "    with open('index.json', 'w') as file:\n",
    "        json.dump(index, file)\n",
    "\n",
    "    return index\n",
    "\n",
    "   \n",
    "def getQuery(keywordString, index):\n",
    "    \"\"\"\n",
    "    the query as list of words from a string. \n",
    "    Strategy applied: lowerCase and stemming \n",
    "    \"\"\"   \n",
    "    print(f\"You search:\\n{keywordString}\")\n",
    "    keywordList = re.sub(\"[^\\w]\", \" \",  keywordString.lower()).split()\n",
    "    cleanedList = [cleanWord(keyword) for keyword in keywordList]\n",
    "    while('' in cleanedList):\n",
    "        cleanedList.remove('')\n",
    "    \n",
    "    #remove keywords that are not in index.keys()\n",
    "    indexKeys = index.keys()\n",
    "    toRemoveList = []\n",
    "    for word in cleanedList:\n",
    "        if word not in indexKeys:\n",
    "            toRemoveList.append(word)\n",
    "\n",
    "    for wordToRemove in toRemoveList:\n",
    "        cleanedList.remove(wordToRemove)\n",
    "   \n",
    "    return cleanedList\n",
    "\n",
    "\n",
    "def search(index, query):\n",
    "    \"\"\"\n",
    "    returns the list of documents who match all the keywords set of documents\n",
    "    matching the query, i.e. that contain all keywords in the query. The return\n",
    "    list is also sort in order of relevance with the tfidf method\n",
    "    \"\"\"   \n",
    "\n",
    "    listOfSets = []\n",
    "    for keyword in query:\n",
    "        # append all urls who match the keyword\n",
    "        listOfSets.append(set(index[keyword].keys()))\n",
    "    if len(listOfSets) == 0:\n",
    "        matchingDocs = set()\n",
    "    else:\n",
    "        #keep only the urls who match all the keyword \n",
    "        matchingDocs = set.intersection(*listOfSets)\n",
    "    \n",
    "    #sort with the tfidf method\n",
    "    return rankResults(matchingDocs, query, index)\n",
    "\n",
    "def getAllWordsOfDocumentCollection(documentCollection):\n",
    "    \"\"\"\n",
    "    returns a list who contains all words of the document collection\n",
    "    \"\"\"\n",
    "    words_lists = []\n",
    "    for line in documentCollection:\n",
    "        words_list = re.sub(\"[^\\w]\", \" \",  line[0]).split()\n",
    "        words_lists.append(words_list)\n",
    "    \n",
    "    return list(itertools.chain.from_iterable(words_lists))\n",
    "\n",
    "def rankResults(matchingDocs, query, index):\n",
    "    \"\"\"\n",
    "    rank the matching documents order of relevance with the tfidf method \n",
    "    \"\"\"\n",
    "    results = []\n",
    "    # sum the score of the words by documents\n",
    "    for doc in matchingDocs:\n",
    "        tfidfSum = 0\n",
    "        for keyword in query:\n",
    "            tfidfSum += index[keyword][doc]\n",
    "        results.append((doc, tfidfSum))\n",
    "    # sort the documents by the previously calculated sums    \n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [doc[0] for doc in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e129cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize document collection (read and create index)\n",
    "path = 'TEDtranscripts.csv'\n",
    "documentCollection = getDocumentCollection(path)\n",
    "  \n",
    "N = len(documentCollection)\n",
    "index = createIndex(documentCollection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdf57e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: 'exit' to stop program, 'search' to search\n",
      "search\n",
      "enter keywords seperated by spaces (e.g. test talk example): work happiness managment burnout\n",
      "You search:\n",
      "work happiness managment burnout\n",
      "Searched among 2467 TED Talks\n",
      "Your TF-IDF ranked search results:\n",
      "https://www.ted.com/talks/yves_morieux_as_work_gets_more_complex_6_rules_to_simplify\n",
      "\n",
      "Type: 'exit' to stop program, 'search' to search\n",
      "search\n",
      "enter keywords seperated by spaces (e.g. test talk example): art compose music creativity human\n",
      "You search:\n",
      "art compose music creativity human\n",
      "Searched among 2467 TED Talks\n",
      "Your TF-IDF ranked search results:\n",
      "https://www.ted.com/talks/mihaly_csikszentmihalyi_on_flow\n",
      "\n",
      "https://www.ted.com/talks/billy_collins_everyday_moments_caught_in_time\n",
      "\n",
      "https://www.ted.com/talks/claron_mcfadden_singing_the_primal_mystery\n",
      "\n",
      "Type: 'exit' to stop program, 'search' to search\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "# interaction loop with the user\n",
    "while (True):\n",
    "    print(\"Type: 'exit' to stop program, 'search' to search\")\n",
    "    userCommand = input()\n",
    "    if(userCommand ==  'exit'):\n",
    "        break\n",
    "    elif(userCommand == 'search'):\n",
    "        query = getQuery(input('enter keywords seperated by spaces (e.g. test talk example): '), index)\n",
    "        print('Searched among ' + str(N) + ' TED Talks')\n",
    "        print('Your TF-IDF ranked search results:')\n",
    "        searchResult = search(index, query)\n",
    "        for URL in searchResult:\n",
    "            print(URL)\n",
    "    else:\n",
    "        print('invalid command')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
